{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from bs4 import SoupStrainer\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import csv\n",
    "from googletrans import Translator\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#индексы загружаются в переменную из файла\n",
    "with open (\"D:/RW/all_regions.txt\",\"r\") as f:\n",
    "    region_url=f.read().split(\",\")\n",
    "print(region_url)\n",
    "len(region_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ea071",
   "metadata": {},
   "source": [
    "# 1 этап. Сбор ссылок на оперативные новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_path): #создание папки \n",
    "    today = datetime.date.today()\n",
    "    folder_name = today.strftime(\"%d_%m_%Y\")\n",
    "    folder_path = os.path.join(folder_path, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    return folder_path\n",
    "\n",
    "def get_news_links(url): #получение ссылок на новости и добавление их в список\n",
    "    set_news=[]\n",
    "    resp = requests.get(url)\n",
    "    data = resp.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    for link in soup.find_all('a'):\n",
    "        set_news.append(link.get('href'))\n",
    "    all_news=[]\n",
    "    for w in set_news:\n",
    "        if re.search(\"\\A/online/news/\\d\", w) is None:\n",
    "            continue\n",
    "        else:\n",
    "            all_news.append(w)\n",
    "    return list(set(all_news))\n",
    "\n",
    "def save_news_links(folder_path, region, all_news): #сохранение полученных ссылок в датафрейм и затем в csv файл\n",
    "    now_datetime = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
    "    news_data = {'link': all_news, 'datetime': [now_datetime] * len(all_news)}\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    file_name = f\"new_results_only_news_{region}.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    news_df.to_csv(file_path, mode='a', index=False, header=True, encoding='utf-8')\n",
    "\n",
    "def main(base_path, region_url):\n",
    "    folder_path = create_folder(base_path)\n",
    "    for region in region_url:\n",
    "        url = f\"https://www.{region}.kp.ru/online\"\n",
    "        all_news = get_news_links(url) #функция поиска нужного \n",
    "        save_news_links(folder_path, region, all_news) #функция сохранения \n",
    "        time.sleep(15)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"D:/RW/data_diplom/new_ssilki/\"\n",
    "    region_url #?\n",
    "    main(base_path, region_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de83eb6",
   "metadata": {},
   "source": [
    "# 2 и 3 этапы. Сбор текстовых материалов, тегов, перевод текста и проведение анализа тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_text(base_path, folder_name):\n",
    "    text_folder = os.path.join(base_path, 'texts', 'mar', folder_name)\n",
    "    ton_folder = os.path.join(base_path, 'ton', 'mar', folder_name)\n",
    "    os.makedirs(text_folder, exist_ok=True)\n",
    "    os.makedirs(ton_folder, exist_ok=True)\n",
    "    return text_folder, ton_folder\n",
    "\n",
    "\n",
    "def extract_text_and_tags(link,region):\n",
    "    urls = 'https://www.'+region+'.kp.ru'+link\n",
    "    print(urls)\n",
    "    result = \"\"\n",
    "    res_tags = \"\"\n",
    "    try:\n",
    "        response = requests.get(urls)\n",
    "        print(response.status_code)\n",
    "    except:\n",
    "        print('Ничего не получилось')\n",
    "    else:\n",
    "        response.encoding = \"utf-8\"\n",
    "        all_text = response.text\n",
    "        soup = BeautifulSoup(all_text)\n",
    "        body_tags = soup.find_all(\"p\", class_=\"sc-1wayp1z-16 dqbiXu\")\n",
    "        pattern_tags = soup.find_all(\"a\", class_='sc-1vxg2pp-0 cXMtmu')\n",
    "        for i in body_tags:\n",
    "            result += i.text + \"\\n\"\n",
    "        for p in pattern_tags:\n",
    "            res_tags += p.text\n",
    "    return result, res_tags   \n",
    "\n",
    "\n",
    "def translation_text(ru_text):\n",
    "    translator = Translator()\n",
    "    en_text = translator.translate(ru_text, src='ru', dest='en')\n",
    "    en_text = str(en_text.text)\n",
    "    return en_text\n",
    "\n",
    "def save_texts(text_folder,result, region, en_text, res_tags):\n",
    "        all_texts_1=list(set(result))\n",
    "        all_texts_dict={'ru_text':[result],'en_text':[en_text],'tags':[res_tags]}\n",
    "        all_texts_df = pd.DataFrame(all_texts_dict)\n",
    "        file_path=os.path.join(text_folder,f\"new_results_texts_{region}.csv\")\n",
    "        all_texts_df.to_csv(file_path, mode='a', index=False, encoding='utf-8')\n",
    "        print('Скачалось')\n",
    "        time.sleep(8)\n",
    "\n",
    "                         \n",
    "def sent_analysis(ton_folder, text_folder, region, folder_name):\n",
    "    texts_file = os.path.join(text_folder, f\"new_results_texts_{region}.csv\")\n",
    "    try:\n",
    "        df1 = pd.read_csv(texts_file)\n",
    "        df2 = df1.iloc[::2]  # Берем каждую вторую строку\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "        ton_texts_dict = {'ton': []}\n",
    "        for i, row in df2.iterrows():\n",
    "            text_ = row['en_text']\n",
    "            string = TextBlob(text_, analyzer=NaiveBayesAnalyzer())\n",
    "            tonalnost = string.sentiment\n",
    "            ton_texts_dict['ton'].append(tonalnost)\n",
    "        ton_texts_df = pd.DataFrame(ton_texts_dict)\n",
    "        df3 = pd.concat([df2, ton_texts_df], axis=1)\n",
    "        df3['ton_kachestvo'], df3['pol'], df3['neg'] = zip(*df3['ton'])\n",
    "        ton_file = os.path.join(ton_folder, f\"results_ton_texts_{region}.csv\")\n",
    "        df3.to_csv(ton_file, mode='a', header=False, index=False, encoding=\"utf-8\")\n",
    "        print('ура, скачалось')\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при анализе тональности: {e}\")\n",
    "                         \n",
    "def main(base_path, folder_name, region_url):\n",
    "    text_folder, ton_folder = create_folder_text(base_path, folder_name)\n",
    "    for region in region_url:\n",
    "        links_file = os.path.join(base_path, 'new_ssilki', folder_name, f\"new_results_only_news_{region}.csv\")\n",
    "        df = pd.read_csv(links_file)\n",
    "        links_list = df.link.to_list()\n",
    "        for link in links_list:\n",
    "            result, res_tags = extract_text_and_tags(link, region)\n",
    "            en_text = translation_text(result)\n",
    "            save_texts(text_folder, result, region, en_text, res_tags)\n",
    "            sent_analysis(ton_folder, text_folder, region, folder_name)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    base_path=\"D:/RW/data_diplom/\"\n",
    "    folder_name=datetime.date.today().strftime(\"%d_%m_%Y\")\n",
    "    region_url\n",
    "    main(base_path,folder_name,region_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7363e",
   "metadata": {},
   "source": [
    "# 4 этап. Приведение данных к виду, удобному для картографирования и счет новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_region(folder_name, tag_values, result_, tags, region):\n",
    "    try:\n",
    "        df = pd.read_csv(f\"d:/rw/data_diplom/ton/dec/{folder_name}/results_ton_texts_{region}.csv\")\n",
    "        new_df = df.iloc[:, [2, 6, 7]]\n",
    "        new_df.columns = ['tag', 'pos', 'neg']\n",
    "\n",
    "        res_pos = new_df.groupby('tag')['pos'].mean()\n",
    "        res_neg = new_df.groupby('tag')['neg'].mean()\n",
    "\n",
    "        meanneg_df = pd.DataFrame({'tag': res_neg.index, 'mean': res_neg.values})\n",
    "        meanpos_df = pd.DataFrame({'tag': res_pos.index, 'mean': res_pos.values})\n",
    "\n",
    "        obsh_df = pd.merge(meanpos_df, meanneg_df, on='tag')\n",
    "        obsh_df.columns = ['tag', 'mean_pos', 'mean_neg']\n",
    "\n",
    "        allowed_values = [tag_values]\n",
    "        obsh_df = obsh_df[obsh_df['tag'].isin(allowed_values)]\n",
    "        obsh_df['count'] = len(new_df[new_df['tag'] == tag_values])\n",
    "        regions = cycle([region])\n",
    "        obsh_df['region'] = [next(regions) for _ in range(len(obsh_df))]\n",
    "\n",
    "        obsh_df = obsh_df.reset_index(drop=True)\n",
    "        os.makedirs(f\"D:/RW/data_diplom/for_map/{result_}/tags/{folder_name}\", exist_ok=True)\n",
    "        obsh_df.to_csv(f\"D:/RW/data_diplom/for_map/{result_}/tags/{folder_name}/results_for_map_{tags}.csv\", mode='a', index=False, encoding=\"utf-8\", header=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_name = '31_12_2023'\n",
    "    tag_values = 'Происшествия'\n",
    "    result_ = 'res_dec'\n",
    "    tags = 'prois'\n",
    "\n",
    "    for region in region_url:\n",
    "        process_region(folder_name, tag_values, result_, tags, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace=r'D:\\RW\\data_diplom\\for_map'\n",
    "month=r'res_dec/tags'\n",
    "tags=['ekon', 'obsh', 'pol', 'prois', 'sport']\n",
    "def get_mean_by_month(workspace, month):\n",
    "    folders= [ f.path for f in os.scandir(os.path.join(workspace, month)) if f.is_dir() and '2023' in f.path ]\n",
    "    \n",
    "    df_ekon=pd.DataFrame(columns=['tag', 'mean_pos', 'mean_neg', 'count', 'region'])\n",
    "    \n",
    "    df_sport=pd.DataFrame(columns=['tag', 'mean_pos', 'mean_neg', 'count', 'region'])\n",
    "    df_prois=pd.DataFrame(columns=['tag', 'mean_pos', 'mean_neg', 'count', 'region'])\n",
    "    df_pol=pd.DataFrame(columns=['tag', 'mean_pos', 'mean_neg', 'count', 'region'])\n",
    "    df_obsh=pd.DataFrame(columns=['tag', 'mean_pos', 'mean_neg', 'count', 'region'])\n",
    "    for folder in folders:\n",
    "\n",
    "        files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "        if len(files)==5:\n",
    "            #ekon\n",
    "            ekon=os.path.join(folder, files[0])\n",
    "            df_ekon_read=pd.read_csv(ekon)\n",
    "            df_ekon_read=df_ekon_read[df_ekon_read['tag']!='tag']\n",
    "            df_ekon=pd.concat([df_ekon, df_ekon_read])\n",
    "            df_ekon.reset_index(drop=True, inplace=True)\n",
    "            #sport\n",
    "            sport=os.path.join(folder, files[4])\n",
    "            df_sport_read=pd.read_csv(sport)\n",
    "            df_sport_read=df_sport_read[df_sport_read['tag']!='tag']\n",
    "            df_sport=pd.concat([df_sport, df_sport_read])\n",
    "            df_sport.reset_index(drop=True, inplace=True)\n",
    "            #prois\n",
    "            prois=os.path.join(folder, files[3])\n",
    "            df_prois_read=pd.read_csv(prois)\n",
    "            df_prois_read=df_prois_read[df_prois_read['tag']!='tag']\n",
    "            df_prois=pd.concat([df_prois, df_prois_read])\n",
    "            df_prois.reset_index(drop=True, inplace=True)\n",
    "            #pol\n",
    "            pol=os.path.join(folder, files[2])\n",
    "            df_pol_read=pd.read_csv(pol)\n",
    "            df_pol_read=df_pol_read[df_pol_read['tag']!='tag']\n",
    "            df_pol=pd.concat([df_pol, df_pol_read])\n",
    "            df_pol.reset_index(drop=True, inplace=True)\n",
    "            #obsh\n",
    "            obsh=os.path.join(folder, files[1])\n",
    "            df_obsh_read=pd.read_csv(obsh)\n",
    "            df_obsh_read=df_obsh_read[df_obsh_read['tag']!='tag']\n",
    "            df_obsh=pd.concat([df_obsh, df_obsh_read])\n",
    "            df_obsh.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df_ekon['count']=pd.to_numeric(df_ekon['count'], errors='coerce')\n",
    "    df_ekon['mean_pos']=pd.to_numeric(df_ekon['mean_pos'], errors='coerce')\n",
    "    df_ekon['mean_neg']=pd.to_numeric(df_ekon['mean_neg'], errors='coerce')\n",
    "    df_ekon_group=df_ekon.groupby(['region']).agg({'count': 'sum', 'mean_pos': 'mean', 'mean_neg': 'mean'})\n",
    "    df_ekon_group['mean_mon']=df_ekon_group['mean_pos']-df_ekon_group['mean_neg']\n",
    "    df_ekon_group.to_csv(os.path.join(workspace, month)+r'/ekon.csv', mode='w', index=True, encoding = \"utf-8\")\n",
    "    # настрой, куда сохранять\n",
    "    df_sport['count']=pd.to_numeric(df_sport['count'], errors='coerce')\n",
    "    df_sport['mean_pos']=pd.to_numeric(df_sport['mean_pos'], errors='coerce')\n",
    "    df_sport['mean_neg']=pd.to_numeric(df_sport['mean_neg'], errors='coerce')\n",
    "    df_sport_group=df_sport.groupby(['region']).agg({'count': 'sum', 'mean_pos': 'mean', 'mean_neg': 'mean'})\n",
    "    df_sport_group['mean_mon']=df_sport_group['mean_pos']-df_sport_group['mean_neg']\n",
    "    df_sport_group.to_csv(os.path.join(workspace, month)+r'/sport.csv', mode='w', index=True, encoding = \"utf-8\")\n",
    "    \n",
    "    df_prois['count']=pd.to_numeric(df_prois['count'], errors='coerce')\n",
    "    df_prois['mean_pos']=pd.to_numeric(df_prois['mean_pos'], errors='coerce')\n",
    "    df_prois['mean_neg']=pd.to_numeric(df_prois['mean_neg'], errors='coerce')\n",
    "    df_prois_group=df_prois.groupby(['region']).agg({'count': 'sum', 'mean_pos': 'mean', 'mean_neg': 'mean'})\n",
    "    df_prois_group['mean_mon']=df_prois_group['mean_pos']-df_prois_group['mean_neg']\n",
    "    df_prois_group.to_csv(os.path.join(workspace, month)+r'/prois.csv', mode='w', index=True, encoding = \"utf-8\")\n",
    "    \n",
    "    df_pol['count']=pd.to_numeric(df_pol['count'], errors='coerce')\n",
    "    df_pol['mean_pos']=pd.to_numeric(df_pol['mean_pos'], errors='coerce')\n",
    "    df_pol['mean_neg']=pd.to_numeric(df_pol['mean_neg'], errors='coerce')\n",
    "    df_pol_group=df_pol.groupby(['region']).agg({'count': 'sum', 'mean_pos': 'mean', 'mean_neg': 'mean'})\n",
    "    df_pol_group['mean_mon']=df_pol_group['mean_pos']-df_pol_group['mean_neg']\n",
    "    df_pol_group.to_csv(os.path.join(workspace, month)+r'/pol.csv', mode='w', index=True, encoding = \"utf-8\")\n",
    "    \n",
    "    df_obsh['count']=pd.to_numeric(df_obsh['count'], errors='coerce')\n",
    "    df_obsh['mean_pos']=pd.to_numeric(df_obsh['mean_pos'], errors='coerce')\n",
    "    df_obsh['mean_neg']=pd.to_numeric(df_obsh['mean_neg'], errors='coerce')\n",
    "    df_obsh_group=df_obsh.groupby(['region']).agg({'count': 'sum', 'mean_pos': 'mean', 'mean_neg': 'mean'})\n",
    "    df_obsh_group['mean_mon']=df_obsh_group['mean_pos']-df_obsh_group['mean_neg']\n",
    "    df_obsh_group.to_csv(os.path.join(workspace, month)+r'/obsh.csv', mode='w', index=True, encoding = \"utf-8\")\n",
    "get_mean_by_month(workspace, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e259d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_geojson(shp_file, csv_file, region_col, name_col, output_file):\n",
    "    csv_data = pd.read_csv(csv_file)\n",
    "    merged_data = shp_file.merge(csv_data, left_on=name_col, right_on=region_col, how='left')\n",
    "    merged_data = merged_data.drop([region_col], axis=1)\n",
    "    target_crs = CRS.from_epsg(4326)\n",
    "    merged_crs_data = merged_data.to_crs(target_crs)\n",
    "    merged_crs_data.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "name_mon = ['dec', 'jan', 'feb', 'mar']\n",
    "name_tag = ['sport', 'ekon', 'prois', 'obsh', 'pol']\n",
    "path_name='D:/RW/data_diplom/for_map'\n",
    "shp_file = gpd.read_file(r'D:\\RW\\data_diplom\\karta\\Redakcion_borders.shp')\n",
    "\n",
    "for a in name_mon:\n",
    "    for i in name_tag:\n",
    "        csv_file = f'{path_name}/res_{a}/tags/{i}.csv'\n",
    "        output_file = f'{path_name}/res_{a}/gis/{i}.geojson'\n",
    "        convert_to_geojson(shp_file, csv_file, 'region', 'gn_name', output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee55e0",
   "metadata": {},
   "source": [
    "### Вспомогательные коды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a105787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найти индекс элемента, если произошло прерываение работы цикла\n",
    "value = input(\"Введите значение элемента: \")\n",
    "\n",
    "# Проверяем, есть ли такой элемент в списке\n",
    "if value in region_url:\n",
    "    # Находим индекс элемента\n",
    "    index = region_url.index(value)\n",
    "    print(\"Индекс элемента\", value, \":\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем первый GeoJSON файл в виде таблицы\n",
    "df = gpd.read_file(r'D:\\RW\\data_diplom\\for_map\\res_dec\\gis\\obrazi_reg_dec_2.geojson')\n",
    "\n",
    "# Присоединяем второй GeoJSON файл по столбцу 'id'\n",
    "df2 = gpd.read_file(r'D:\\RW\\data_diplom\\for_map\\res_jan\\gis\\obrazi_reg_jan.geojson')\n",
    "merged = df.merge(df2, on='gn_name', how='left')\n",
    "\n",
    "# Присоединяем третий GeoJSON файл по столбцу 'name'\n",
    "df3 = gpd.read_file(r'D:\\RW\\data_diplom\\for_map\\res_feb\\gis\\obrazi_reg_feb.geojson')\n",
    "merged = merged.merge(df3, on='gn_name', how='left')\n",
    "\n",
    "merged.to_excel(r'D:\\RW\\data_diplom\\for_map\\all_im.xlsx')\n",
    "\n",
    "df = pd.read_excel(r'D:\\RW\\data_diplom\\for_map\\all_im.xlsx')\n",
    "df4 = gpd.read_file(r'D:\\RW\\data_diplom\\for_map\\res_mar\\gis\\obrazi_reg_mar.geojson')\n",
    "merged = df.merge(df4, on='gn_name', how='left')\n",
    "\n",
    "merged.to_excel(r'D:\\RW\\data_diplom\\for_map\\all_im_2.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
